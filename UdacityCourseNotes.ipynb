{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Machine Learning\n",
    "\n",
    "These are notes made following the Udacity course, an intro to ML.\n",
    "\n",
    "The code snippets are the key lines for each concept being worked on, but mostly can't be run in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Naive Bayes\n",
    "\n",
    "This is a supervised classification algorithm. The idea is based on figuring out, which source something came from based on the probabilities that thing does stuff.\n",
    "\n",
    "Think of text analysis, two people who send emails with certain word probabilities. Given an email with certain words, you can figure out the probability of it coming from each one. It's naive because it ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# These numbers are random, so this isn't a very useful classifier\n",
    "features_train=np.array([[1,1],[5,8],[7,0],[1,5],[4,5],[1,2],[7,7],[3,1],[0,5]])\n",
    "labels_train=np.array([1,2,1,1,2,2,1,1,1])\n",
    "\n",
    "\n",
    "# Import gaussian naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Generate a classifier\n",
    "clf=GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print(clf.predict([[2,3]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the full code written at the end of lesson 1 (it won't run on it's own) \n",
    "\n",
    "The timing measurements show that the prediction is much faster than the training section, by like 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Support Vector Machines (SVMs)\n",
    "\n",
    "Another Classifier. Very popular and really good, quite new.\n",
    "\n",
    "Broadly SVMs maximise the margin between the sets, but can tolerate some degree of outliers.\n",
    "The important thing about how we use these is kernels. A Kernel takes a low dimensional system to a high dimensional system, which then allows the SVM to find a linear seperation.\n",
    "\n",
    "They are cubic with data size, so are difficult to use on large datasets. They are also very prone to noise, and vulnerable to overfitting.\n",
    "\n",
    "A few important parameters:\n",
    "- Kernel\n",
    "- Gamma\n",
    "- C\n",
    "\n",
    "Control of these parameters is important to avoid overfitting.\n",
    "\n",
    "### C\n",
    "Controls the tradeoff between having points correct and having a smooth boundary. A large value of C means more points will be correct (fewer points allowed to be outliers)\n",
    "\n",
    "### Gamma\n",
    "Defines the reach of each training boundary. Low values far, high values close reach.\n",
    "A low value and high reach tends to smooth the decision boundary and effectively reduces the impact of the values close to the boundary relative to the many other points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 Decision Trees\n",
    "\n",
    "Allow asking multiple linear questions, one after the other. These are prone to overfit, but very simple process. You can build very big classifiers with these.\n",
    "\n",
    "There are several important parameters for tuning the accuracy and avoiding overfitting, such as:\n",
    "\n",
    "__MinSample Split__\n",
    "This governs how small you go before you stop splitting\n",
    "Default 2, but this can be reduced to avoid overfitting\n",
    "\n",
    "### Entropy\n",
    "The DT makes decisions looking at splitting the data to create subsets, which are as pure as possible.\n",
    "Entropy goes between 0 (All one class) and 1.0 (Evenly split between classes. \n",
    "The entropy is calculated as:\n",
    "\n",
    "$$-\\Sigma_{P_{i}} P_{i}log_{2}(P_{i})$$\n",
    "\n",
    "where $P_{i}$ is the fraction of the total for each class i.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "The decision tree acts to maximise information gain, which is the entropy of the parent minus the weighted average of the entropy of the children that could be created.\n",
    "\n",
    "### Bias-Variance dilemma\n",
    "\n",
    "High bias means it ignores data, doesn't learn.\n",
    "High variance means it learns very quickly, so doesn't retain much.\n",
    "The balance between these two is really key to optimising any algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5: Choose your own: Adaboost\n",
    "\n",
    "Adaboost (adaptive boosting), like Random Forest is an ensemble method, which combines outputs from various weak classifiers to give a better classification. As it goes on, it identifies hard to classify data and later classifiers focus on these.\n",
    "\n",
    "Each iteration gives a classification and this is evaluated. Wrongly classified datapoints have the weight increased, while correctly classified ones have their weight decreased and this modified dataset is then evaluated again. This process is iterated and these are then combined. When combining the stumps, the weight of the stumps are taken into account\n",
    "\n",
    "__Basic ideas:__\n",
    "\n",
    "1. Combines weak learners, usually stumps (a tree with just two leaves)\n",
    "2. Each stump gets a weighting\n",
    "3. Each stump takes the previous mistakes into account\n",
    "\n",
    "__Detailed:__\n",
    "\n",
    "1. The weight for getting each datapoint right is initially equal.\n",
    "2. Find the best initial weak classifier\n",
    "3. Give the stump a weight, sum of weights of incorrect samples (low is good)\n",
    "4. Negative weight means the answers are reversed\n",
    "5. New weights for incorrectly classified samples are increased by $ sample weight \\times e^{Stump weight}$\n",
    "6. New weights for correctly classified samples are decreased by $ sample weight \\times e^{-Stump weight}$\n",
    "7. These weights then need normalising to one\n",
    "8. A new collection of samples is built by randomly picking from the original, but using the weights to give the probability of each sample. Hence previously wrongly classified samples are likely to appear multiple times.\n",
    "9. We then go back to step 1 with the new samples, as many times as we like. \n",
    "10. We then use the weighted average of this classification.\n",
    "\n",
    "n_estimators is a key parameter. A large number means that you're likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf= AdaBoostClassifier(n_estimators=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6: Datasets and Questions\n",
    "### Finding Fraudsters in Enron email\n",
    "\n",
    "Persons of interest (POI) are people who were indicted or settled out of court.\n",
    "\n",
    "- 35 people, 30 at Enron. \n",
    "- 150 people emails in total.\n",
    "- Only 4 email boxes of those POI.\n",
    "\n",
    "This is not necessarily a complete list. Many people may not have been caught and many people\n",
    "\n",
    "What's more, we don't really know whether there are enough of these people to give the accuracy we want. Accuracy vs. Training set size needs consideration. Using more data is often a better way of improving outcomes, more than \n",
    "\n",
    "\n",
    "This lesson just makes you explore the data. One point highlighted was that if data comes from different sources you have to be careful about merging it, without introducing biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7: Regression\n",
    "\n",
    "Previous work has had a constrained output, a discrete, binary answer. We now look at continuous supervised learning, where the output is no longer discrete. This implies there's an order, like age or salary.\n",
    "\n",
    "We are trying to find an equation for a line of best fit to the data. These algorithms work to minimise the sum of square errors. The squared comes in to ensure that there is one unique solution, as well as making the computation easier.\n",
    "\n",
    "- Ordinary least squares\n",
    "- Gradient descent\n",
    "\n",
    "We evaluate the fit with R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg=LinearRegression()\n",
    "\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print(\"Slope is: \\t\"+ str(reg.coef_))\n",
    "print(\"Intercept is: \\t\"+ str(reg.intercept_))\n",
    "print(\"R-squared for training data is: \\t\"+ str(reg.score(feature_train, target_train)))\n",
    "print(\"R-squared for test data is: \\t\"+ str(reg.score(feature_test, target_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8: Outliers in Regression\n",
    "\n",
    "As we saw in Lesson 8, outliers can have a very strong effect on regression. swapping an outlier from training to test sets can make a huge difference to the prediction and the score.\n",
    "\n",
    "Outliers are often problems that we want to ignore, although sometimes they could be the most interesting/key points!\n",
    "\n",
    "A simple process to detect outliers is:\n",
    "\n",
    "1. train\n",
    "2. remove data with highest residual error (say 10% highest)\n",
    "3. train again\n",
    "\n",
    "2 and 3 could be iterated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        Clean away the 10% of points that have the largest\n",
    "        residual errors (difference between the prediction\n",
    "        and the actual net worth).\n",
    "\n",
    "        Return a list of tuples named cleaned_data where \n",
    "        each tuple is of the form (age, net_worth, error).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    cleaned_data = []\n",
    "\n",
    "    errors = abs(np.subtract(net_worths,predictions))\n",
    "    maxErr = np.percentile(errors,90)\n",
    "    \n",
    "    for error,age,net_worth in zip(errors,ages,net_worths):\n",
    "        if error<=maxErr:\n",
    "            cleaned_data.append((age,net_worth,error))\n",
    "    print (len(cleaned_data))\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
