{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Machine Learning\n",
    "\n",
    "These are notes made following the Udacity course, an intro to ML.\n",
    "\n",
    "The code snippets are the key lines for each concept being worked on, but mostly can't be run in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Naive Bayes\n",
    "\n",
    "This is a supervised classification algorithm. The idea is based on figuring out, which source something came from based on the probabilities that thing does stuff.\n",
    "\n",
    "Think of text analysis, two people who send emails with certain word probabilities. Given an email with certain words, you can figure out the probability of it coming from each one. It's naive because it ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# These numbers are random, so this isn't a very useful classifier\n",
    "features_train=np.array([[1,1],[5,8],[7,0],[1,5],[4,5],[1,2],[7,7],[3,1],[0,5]])\n",
    "labels_train=np.array([1,2,1,1,2,2,1,1,1])\n",
    "\n",
    "\n",
    "# Import gaussian naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Generate a classifier\n",
    "clf=GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print(clf.predict([[2,3]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the full code written at the end of lesson 1 (it won't run on it's own) \n",
    "\n",
    "The timing measurements show that the prediction is much faster than the training section, by like 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Support Vector Machines (SVMs)\n",
    "\n",
    "Another Classifier. Very popular and really good, quite new.\n",
    "\n",
    "Broadly SVMs maximise the margin between the sets, but can tolerate some degree of outliers.\n",
    "The important thing about how we use these is kernels. A Kernel takes a low dimensional system to a high dimensional system, which then allows the SVM to find a linear seperation.\n",
    "\n",
    "They are cubic with data size, so are difficult to use on large datasets. They are also very prone to noise, and vulnerable to overfitting.\n",
    "\n",
    "A few important parameters:\n",
    "- Kernel\n",
    "- Gamma\n",
    "- C\n",
    "\n",
    "Control of these parameters is important to avoid overfitting.\n",
    "\n",
    "### C\n",
    "Controls the tradeoff between having points correct and having a smooth boundary. A large value of C means more points will be correct (fewer points allowed to be outliers)\n",
    "\n",
    "### Gamma\n",
    "Defines the reach of each training boundary. Low values far, high values close reach.\n",
    "A low value and high reach tends to smooth the decision boundary and effectively reduces the impact of the values close to the boundary relative to the many other points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 Decision Trees\n",
    "\n",
    "Allow asking multiple linear questions, one after the other. These are prone to overfit, but very simple process. You can build very big classifiers with these.\n",
    "\n",
    "There are several important parameters for tuning the accuracy and avoiding overfitting, such as:\n",
    "\n",
    "__MinSample Split__\n",
    "This governs how small you go before you stop splitting\n",
    "Default 2, but this can be reduced to avoid overfitting\n",
    "\n",
    "### Entropy\n",
    "The DT makes decisions looking at splitting the data to create subsets, which are as pure as possible.\n",
    "Entropy goes between 0 (All one class) and 1.0 (Evenly split between classes. \n",
    "The entropy is calculated as:\n",
    "\n",
    "$$-\\Sigma_{P_{i}} P_{i}log_{2}(P_{i})$$\n",
    "\n",
    "where $P_{i}$ is the fraction of the total for each class i.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "The decision tree acts to maximise information gain, which is the entropy of the parent minus the weighted average of the entropy of the children that could be created.\n",
    "\n",
    "### Bias-Variance dilemma\n",
    "\n",
    "High bias means it ignores data, doesn't learn.\n",
    "High variance means it learns very quickly, so doesn't retain much.\n",
    "The balance between these two is really key to optimising any algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5: Choose your own: Adaboost\n",
    "\n",
    "Adaboost (adaptive boosting), like Random Forest is an ensemble method, which combines outputs from various weak classifiers to give a better classification. As it goes on, it identifies hard to classify data and later classifiers focus on these.\n",
    "\n",
    "Each iteration gives a classification and this is evaluated. Wrongly classified datapoints have the weight increased, while correctly classified ones have their weight decreased and this modified dataset is then evaluated again. This process is iterated and these are then combined. When combining the stumps, the weight of the stumps are taken into account\n",
    "\n",
    "__Basic ideas:__\n",
    "\n",
    "1. Combines weak learners, usually stumps (a tree with just two leaves)\n",
    "2. Each stump gets a weighting\n",
    "3. Each stump takes the previous mistakes into account\n",
    "\n",
    "__Detailed:__\n",
    "\n",
    "1. The weight for getting each datapoint right is initially equal.\n",
    "2. Find the best initial weak classifier\n",
    "3. Give the stump a weight, sum of weights of incorrect samples (low is good)\n",
    "4. Negative weight means the answers are reversed\n",
    "5. New weights for incorrectly classified samples are increased by $ sample weight \\times e^{Stump weight}$\n",
    "6. New weights for correctly classified samples are decreased by $ sample weight \\times e^{-Stump weight}$\n",
    "7. These weights then need normalising to one\n",
    "8. A new collection of samples is built by randomly picking from the original, but using the weights to give the probability of each sample. Hence previously wrongly classified samples are likely to appear multiple times.\n",
    "9. We then go back to step 1 with the new samples, as many times as we like. \n",
    "10. We then use the weighted average of this classification.\n",
    "\n",
    "n_estimators is a key parameter. A large number means that you're likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf= AdaBoostClassifier(n_estimators=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6: Datasets and Questions\n",
    "### Finding Fraudsters in Enron email\n",
    "\n",
    "Persons of interest (POI) are people who were indicted or settled out of court.\n",
    "\n",
    "- 35 people, 30 at Enron. \n",
    "- 150 people emails in total.\n",
    "- Only 4 email boxes of those POI.\n",
    "\n",
    "This is not necessarily a complete list. Many people may not have been caught and many people\n",
    "\n",
    "What's more, we don't really know whether there are enough of these people to give the accuracy we want. Accuracy vs. Training set size needs consideration. Using more data is often a better way of improving outcomes, more than \n",
    "\n",
    "\n",
    "This lesson just makes you explore the data. One point highlighted was that if data comes from different sources you have to be careful about merging it, without introducing biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7: Regression\n",
    "\n",
    "Previous work has had a constrained output, a discrete, binary answer. We now look at continuous supervised learning, where the output is no longer discrete. This implies there's an order, like age or salary.\n",
    "\n",
    "We are trying to find an equation for a line of best fit to the data. These algorithms work to minimise the sum of square errors. The squared comes in to ensure that there is one unique solution, as well as making the computation easier.\n",
    "\n",
    "- Ordinary least squares\n",
    "- Gradient descent\n",
    "\n",
    "We evaluate the fit with R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg=LinearRegression()\n",
    "\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print(\"Slope is: \\t\"+ str(reg.coef_))\n",
    "print(\"Intercept is: \\t\"+ str(reg.intercept_))\n",
    "print(\"R-squared for training data is: \\t\"+ str(reg.score(feature_train, target_train)))\n",
    "print(\"R-squared for test data is: \\t\"+ str(reg.score(feature_test, target_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8: Outliers in Regression\n",
    "\n",
    "As we saw in Lesson 8, outliers can have a very strong effect on regression. swapping an outlier from training to test sets can make a huge difference to the prediction and the score.\n",
    "\n",
    "Outliers are often problems that we want to ignore, although sometimes they could be the most interesting/key points!\n",
    "\n",
    "A simple process to detect outliers is:\n",
    "\n",
    "1. train\n",
    "2. remove data with highest residual error (say 10% highest)\n",
    "3. train again\n",
    "\n",
    "2 and 3 could be iterated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlierCleaner(predictions, ages, net_worths):\n",
    "    \"\"\"\n",
    "        Clean away the 10% of points that have the largest\n",
    "        residual errors (difference between the prediction\n",
    "        and the actual net worth).\n",
    "\n",
    "        Return a list of tuples named cleaned_data where \n",
    "        each tuple is of the form (age, net_worth, error).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    cleaned_data = []\n",
    "\n",
    "    errors = abs(np.subtract(net_worths,predictions))\n",
    "    maxErr = np.percentile(errors,90)\n",
    "    \n",
    "    for error,age,net_worth in zip(errors,ages,net_worths):\n",
    "        if error<=maxErr:\n",
    "            cleaned_data.append((age,net_worth,error))\n",
    "    print (len(cleaned_data))\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9: Clustering\n",
    "\n",
    "Unsupervised learning is for datasets without labels. Clustering is one method, which determines groups. e.g. Netflix wanting to know which movies to suggest. Do they go in your cluster?\n",
    "\n",
    "### k-means algorithm\n",
    "\n",
    "Each cluster could be given a cluster centre and it is this idea that is used in k-means.\n",
    "\n",
    "1. Determine how many clusters to look for (n).\n",
    "2. Create n random cluster centres\n",
    "3. Assign each point to the nearest cluster centre\n",
    "4. Optimise: Move the centres to minimise the sum of quadratic distances for each cluster\n",
    "5. Iterate 3. and 4.\n",
    "\n",
    "Need to determine the number of clusters and can also determine the max number of iterations and the number of initialisations it incorporates.\n",
    "\n",
    "The initial conditions are really key to the end result. There is always a risk of ending up in a local minima, so we avoid this by running the algorithm several times and determining the best outcome.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf = KMeans(n_clusters=2)\n",
    "clf.fit(finance_features)\n",
    "\n",
    "pred=clf.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 10: Feature Scaling\n",
    "\n",
    "Features that you want to use should be on a similar scale, i.e. normalised between 0 and 1.\n",
    "\n",
    "$$ x' = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Outliers will distort this scaling function quite severely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "finance_features_scaled=scaler.fit_transform(finance_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 11: Text Learning\n",
    "\n",
    "Text is key to many web \n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Making a frequency count for each key words, that you can use to define your features.\n",
    "\n",
    "Stopwords are words that you want to ignore, e.g. \"the\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'one', 'second', 'third']\n",
      "[[1 1 0 0 0]\n",
      " [2 0 0 1 0]\n",
      " [0 0 1 0 1]\n",
      " [1 1 0 0 0]]\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "StopW=stopwords.words('english')\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?']\n",
    "vectoriser = CountVectorizer(stop_words=StopW)\n",
    "X = vectoriser.fit_transform(corpus)\n",
    "print(vectoriser.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "print (vectoriser.vocabulary_.get(\"first\"))\n",
    "print (vectoriser.vocabulary_.get(\"first\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(StopW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Equally, lots of words form differently, but are basically the same. e.g. response, responded, responsive.\n",
    "\n",
    "These can all be grouped into one stem e.g. \"respon\". This is normally taken from a tool made by professional linguists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'string', 'method', 'translat', 'return', 'a', 'copi', 'of', 'the', 'string', 'in', 'which', 'all', 'charact', 'have', 'been', 'translat', 'use', 'tabl', 'construct', 'with', 'the', 'maketran', 'function', 'in', 'the', 'string', 'modul', 'option', 'delet', 'all', 'charact', 'found', 'in', 'the', 'string', 'deletechar']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "text_string='Python string method translate() returns a copy of the string in which all characters have been translated using table (constructed with the maketrans() function in the string module), optionally deleting all characters found in the string deletechars.'\n",
    "\n",
    "### remove punctuation\n",
    "text_string = text_string.translate(text_string.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words=text_string.split(\" \")\n",
    "\n",
    "stems=[]\n",
    "for word in words:\n",
    "    stems.append(stemmer.stem(word))\n",
    "\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF\n",
    "\n",
    "Term frequency - like bag if words\n",
    "Inverse document frequency - Compares the document to how common a word is in this document compared to the rest of the corpus\n",
    "\n",
    "This rates the rare words higher than the more common words.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "StopW=stopwords.words('english')\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?']\n",
    "vectoriser = CountVectorizer(stop_words=StopW)\n",
    "X = vectoriser.fit_transform(corpus)\n",
    "print(vectoriser.get_feature_names())\n",
    "print(X.toarray())\n",
    "\n",
    "print (vectoriser.vocabulary_.get(\"first\"))\n",
    "print (vectoriser.vocabulary_.get(\"first\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "StopW=stopwords.words('english')\n",
    "\n",
    "vectoriser = TfidfVectorizer(stop_words=StopW)\n",
    "X = vectoriser.fit_transform(word_data)\n",
    "\n",
    "print(vectoriser.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12: Feature Selection\n",
    "\n",
    "\"Make things as simple as possible, but no simpler.\" Albert Einstein\n",
    "\n",
    "Choosing the features used to train your algorithms is vital to making them work, so this needs to be done very carefully.\n",
    "\n",
    "### Adding a new feature\n",
    "\n",
    "- Use intuition: what might be a useful indicator of what your looking for\n",
    "- Code it up\n",
    "- Visualise\n",
    "- Learn and repeat\n",
    "\n",
    "Beware of bugs! If it looks too good to be true, you are probably tracking labels inaccurately.\n",
    "\n",
    "### Getting rid of a feature\n",
    "\n",
    "Why get rid?\n",
    "- It's noisy\n",
    "- It's highly correlated with another feature\n",
    "- It slows down the computation time\n",
    "\n",
    "Features != Information\n",
    "\n",
    "There are several go-to methods of automatically selecting your features in sklearn. Many of them fall under the umbrella of univariate feature selection, which treats each feature independently and asks how much power it gives you in classifying or regressing.\n",
    "\n",
    "There are two big univariate feature selection tools in sklearn: SelectPercentile and SelectKBest. The difference is pretty apparent by the names: SelectPercentile selects the X% of features that are most powerful (where X is a parameter) and SelectKBest selects the K features that are most powerful (where K is a parameter).\n",
    "\n",
    "### Regularisation\n",
    "\n",
    "There is a sweet spot between too few features (high bias) and too many (high variance). Finding the features that give us this sweet spot can be done automatically through regularisation.\n",
    "\n",
    "#### Lasso Regression\n",
    "\n",
    "Want to minimise SSE, but also the number of features. The gain of a new feature has to be large enough to counter a penalty for including that additional feature.\n",
    "\n",
    "Each feature is assessed and the coefficient of regression for each feature is set to zero if those features are not informative enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
