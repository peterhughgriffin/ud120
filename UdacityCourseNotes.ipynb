{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Machine Learning\n",
    "\n",
    "These are notes made following the Udacity course, an intro to ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Naive Bayes\n",
    "\n",
    "This is a supervised classification algorithm. The idea is based on figuring out, which source something came from based on the probabilities that thing does stuff.\n",
    "\n",
    "Think of text analysis, two people who send emails with certain word probabilities. Given an email with certain words, you can figure out the probability of it coming from each one. It's naive because it ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# These numbers are random, so this isn't a very useful classifier\n",
    "features_train=np.array([[1,1],[5,8],[7,0],[1,5],[4,5],[1,2],[7,7],[3,1],[0,5]])\n",
    "labels_train=np.array([1,2,1,1,2,2,1,1,1])\n",
    "\n",
    "\n",
    "# Import gaussian naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Generate a classifier\n",
    "clf=GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print(clf.predict([[2,3]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the full code written at the end of lesson 1 (it won't run on it's own) \n",
    "\n",
    "The timing measurements show that the prediction is much faster than the training section, by like 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3: Support Vector Machines (SVMs)\n",
    "\n",
    "Another Classifier. Very popular and really good, quite new.\n",
    "\n",
    "Broadly SVMs maximise the margin between the sets, but can tolerate some degree of outliers.\n",
    "The important thing about how we use these is kernels. A Kernel takes a low dimensional system to a high dimensional system, which then allows the SVM to find a linear seperation.\n",
    "\n",
    "They are cubic with data size, so are difficult to use on large datasets. They are also very prone to noise, and vulnerable to overfitting.\n",
    "\n",
    "A few important parameters:\n",
    "- Kernel\n",
    "- Gamma\n",
    "- C\n",
    "\n",
    "Control of these parameters is important to avoid overfitting.\n",
    "\n",
    "### C\n",
    "Controls the tradeoff between having points correct and having a smooth boundary. A large value of C means more points will be correct (fewer points allowed to be outliers)\n",
    "\n",
    "### Gamma\n",
    "Defines the reach of each training boundary. Low values far, high values close reach.\n",
    "A low value and high reach tends to smooth the decision boundary and effectively reduces the impact of the values close to the boundary relative to the many other points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print (\"training time:\", round(time()-t0, 3), \"s\")\n",
    "\n",
    "t1 = time()\n",
    "pred = clf.predict(features_test)\n",
    "print (\"prediction time:\", round(time()-t1, 3), \"s\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Accuracy = accuracy_score(labels_test, pred)\n",
    "\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
